{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kerasを用いたCIFAR-10 チュートリアル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. はじめに"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このチュートリアルでは、[CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html)と呼ばれる画像分類問題を扱います。CIFAR-10は5万枚の32x32ピクセルのカラーの画像データと、それを分類する10個のラベル「飛行機、自動車、鳥、猫、鹿、犬、蛙、馬、船、トラック」で成り立っています。より細分化されたラベルに分類するCIFAR-100というものもあり、こちらはラベルが100個ついています。どちらも基本的には同じものなので、今回はCIFAR-10を使ってみます。\n",
    "\n",
    "また、実装には[Keras](https://keras.io/ja/)を使います。Kerasは教師なし学習には使えませんが、CIFAR-10やMNISTを代表する典型的な機械学習の問題を扱う上においては、Kerasで十分だと思います。さらにKerasはシンプルなネットワーク（といっても一方向のネットワークだけではなく、分岐するようなもの、RNNなど）に関しては非常に簡単に実装できるので、初心者の方も（生の[Tensorflow](https://www.tensorflow.org)よりは）何やっているか理解しやすいと思います。このチュートリアルでは、Kerasの実装法も（簡単にですが）説明しているので、Kerasを知らない方も読み進めることができるのではないかと思います。詰まった際は[公式ドキュメント](https://keras.io/ja/)を参照ください。\n",
    "\n",
    "このチュートリアルは私が[大学の講義](https://github.com/hiroyuki827/deep_learning_in_physics_research_SS17)で学んだ内容をベースにしているので、私自身が理解するのに時間がかかったところなどは余計に説明を加えました。全体的に冗長な感じは否めませんが、あえて残すことで似たような悩みを持つ方に役立つのではないかと考えています。\n",
    "\n",
    "<u>予備知識&想定読者</u>\n",
    "- 深くは理解できていないが、[MNIST for beginners](https://www.tensorflow.org/get_started/mnist/beginners)はやってみた。が、その後が続かない。\n",
    "- 機械学習の背景については一通り勉強してみた（「[ゼロから作るDeep Learning](https://www.oreilly.co.jp/books/9784873117584/)」)\n",
    "- Kerasを使ってCNNをやってみたい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>動作環境</u>\n",
    "\n",
    "出来る限りパワーのあるパソコンがあればいいですが、もしなければクラウドのサービス（たとえば[Google Colaboratry](https://qiita.com/tomo_makes/items/f70fe48c428d3a61e131)など）をどうぞライブラリのバージョンは以下の通り:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.1.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### このノートを始める前に\n",
    "<u>注意 その0</u> `dlt`パッケージについては、`pip install dlt`でダウンロードしてください。dltパッケージの使い方については、別途[`dltパッケージの使い方/`](https://github.com/hiroyuki827/deep_learning_tools/blob/master/dltパッケージの使い方/dliprパッケージの使い方.ipynb)を参照ください。Jupyter上では以下を実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>注意 その1</u> \n",
    "いくつか勉強不足の点があるので、間違いを見つけられた場合はご指摘いただけると嬉しいです。これはissueページにて受け付けております。なお、各ライブラリのバージョンについては以下のようになっています。\n",
    "\n",
    "<u>注意 その2</u> また、Jupyter notebookの行番号が入っていないところもありますが、基本的には上から読んでいっていただければと思います。本論については実際に実行し、結果をHTMLで記述しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. データの読み込み\n",
    "\n",
    "ここではdltを用いたデータの読み込みについて議論します。CIFAR-10のデータセットがどういう風に成り立っているのかを詳しく調べた後、one-hotベクトルについて説明しました。このあたりはご存じの方は軽く流していただいて構いません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、必要なパッケージ等をインポートします。画像を扱うためには、その向きを考慮するためにCNN (Convolutional Neuron Network) を使う必要があるので、それを考慮して以下のようにインポートします。ここで`os`モジュールは、ファイルの保存ディレクトリの作成に使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlt\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にCIFAR-10のデータフォルダから必要なデータを読み込みます。後の学習結果の可視化を使うため、dltパッケージを用いてダウンロードします。\n",
    "\n",
    "**コメント** データセットは`~/.keras/dataset`以下にダウンロードされます。ダウンロードにはTensorflowのコードを用いています。詳しくはdltパッケージ内の`cifar.py`参照。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading CIFAR-10 dataset\n"
     ]
    }
   ],
   "source": [
    "data = dlt.cifar.load_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 1)\n",
      "(10000, 32, 32, 3)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "#学習用の画像とラベル\n",
    "print(data.train_images.shape)\n",
    "print(data.train_labels.shape)\n",
    "\n",
    "#テスト用の画像とラベル\n",
    "print(data.test_images.shape)\n",
    "print(data.test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は、訓練用として5万枚の画像ファイルと、テスト用の1万枚の画像ファイルを使います。それぞれの画像は32x32の1024画素で構成されています。要するに\n",
    "\n",
    "`(画像, 行(or列)方向のピクセル, 列(or行)方向のピクセル, RGB+黒)`\n",
    "\n",
    "のようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 読み込んだデータの内容\n",
    "\n",
    "ここで少し本筋からそれますが、読み込んだデータがどのようなものかを確認しておきたいと思います。\n",
    "#### data.train_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一つ目の添字`50000`には、各画像のRGB値が入っています。一つ目の画像のデータは以下のように見れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 59  62  63]\n",
      "  [ 43  46  45]\n",
      "  [ 50  48  43]\n",
      "  ...\n",
      "  [158 132 108]\n",
      "  [152 125 102]\n",
      "  [148 124 103]]\n",
      "\n",
      " [[ 16  20  20]\n",
      "  [  0   0   0]\n",
      "  [ 18   8   0]\n",
      "  ...\n",
      "  [123  88  55]\n",
      "  [119  83  50]\n",
      "  [122  87  57]]\n",
      "\n",
      " [[ 25  24  21]\n",
      "  [ 16   7   0]\n",
      "  [ 49  27   8]\n",
      "  ...\n",
      "  [118  84  50]\n",
      "  [120  84  50]\n",
      "  [109  73  42]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[208 170  96]\n",
      "  [201 153  34]\n",
      "  [198 161  26]\n",
      "  ...\n",
      "  [160 133  70]\n",
      "  [ 56  31   7]\n",
      "  [ 53  34  20]]\n",
      "\n",
      " [[180 139  96]\n",
      "  [173 123  42]\n",
      "  [186 144  30]\n",
      "  ...\n",
      "  [184 148  94]\n",
      "  [ 97  62  34]\n",
      "  [ 83  53  34]]\n",
      "\n",
      " [[177 144 116]\n",
      "  [168 129  94]\n",
      "  [179 142  87]\n",
      "  ...\n",
      "  [216 184 140]\n",
      "  [151 118  84]\n",
      "  [123  92  72]]]\n"
     ]
    }
   ],
   "source": [
    "print(data.train_images[0]) #一つ目の添字`50000`のうち、一つ目の画像のデータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "二つ目と三つ目(`32, 32`)は、各画像のピクセルを指定するものです。各画像は32x32で構成されているので、この二つの添字を指定すると、どのピクセルを見ているかがわかります。たとえば一つ目の画像の、一つ目のピクセルを見たかったらまず、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  62  63]\n",
      " [ 43  46  45]\n",
      " [ 50  48  43]\n",
      " [ 68  54  42]\n",
      " [ 98  73  52]\n",
      " [119  91  63]\n",
      " [139 107  75]\n",
      " [145 110  80]\n",
      " [149 117  89]\n",
      " [149 120  93]\n",
      " [131 103  77]\n",
      " [125  99  76]\n",
      " [142 115  91]\n",
      " [144 112  86]\n",
      " [137 105  79]\n",
      " [129  97  71]\n",
      " [137 106  79]\n",
      " [134 106  76]\n",
      " [124  97  64]\n",
      " [139 113  78]\n",
      " [139 112  75]\n",
      " [133 105  69]\n",
      " [136 105  74]\n",
      " [139 108  77]\n",
      " [152 120  89]\n",
      " [163 131 100]\n",
      " [168 136 108]\n",
      " [159 129 102]\n",
      " [158 130 104]\n",
      " [158 132 108]\n",
      " [152 125 102]\n",
      " [148 124 103]]\n",
      "(32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(data.train_images[0][0]) # [画像の番号][ピクセルの行or列方向指定]\n",
    "print(data.train_images[0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "として行or列方向のピクセルの情報を出します。これは32個あります。そして"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59 62 63]\n"
     ]
    }
   ],
   "source": [
    "print(data.train_images[0][0][0]) # [画像の番号][ピクセルの行or列方向指定][ピクセルの列or行方向指定]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とします。ここで`3`はRGBの自由度3を表します。つまり、このピクセルはRGBが`[59 62 63]`で構成されているということがわかります。\n",
    "\n",
    "以上の考察ではそれぞれの画像を特定しましたが、画像を特定せずに、ピクセルすべてを見たいときは添字2つめと3つめを指定してやれば良いです。この場合、各ピクセルの3色の自由度も含まれることになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### data.train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にラベルについて見ていきます。はじめの１枚の画像を調べると、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6]\n"
     ]
    }
   ],
   "source": [
    "print(data.train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と得られます。これは`cifar.py`において\n",
    "\n",
    "```\n",
    "cifar10_labels = np.array([\n",
    "    'airplane',\n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'])\n",
    "```\n",
    "とラベルが定義されているので、この画像は`frog`であることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data.test_images`, `data.test_labels`についても同様にしてわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "話をもとに戻しましょう。ここで、訓練データの画像を`examples.png`ファイルとして見てみます。(ここでは保存されたファイルを表示させています。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_examples(data, fname='examples.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='examples.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "入力値の訓練用データの変数、テスト用データの変数をそれぞれ`X_train`, `X_test`とおくことにしましょう。（単回帰解析で変数を$x$と置くことに対応します。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 training samples\n",
      "10000 test samples\n",
      "(50000, 32, 32, 3)\n",
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = data.train_images.reshape([-1, 32, 32, 3])\n",
    "X_test = data.test_images.reshape([-1, 32, 32, 3])\n",
    "\n",
    "print('%i training samples' % X_train.shape[0])\n",
    "print('%i test samples' % X_test.shape[0])\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※ もともとのデータ`data.train_images`, `data.test_labels`と同じサイズなのでreshapeする必要はありませんが、とりあえずしておきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### データ量のラベルに対する分布\n",
    "\n",
    "今回の場合はデータセットが用意されているので気にする必要はありませんが、自分でデータセットを作る際は、各ラベルに対してデータの偏りがないようにする必要があります。これについても次のようにチェックすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Value: 5000\n",
      "Median Value: 5000.0\n",
      "Variance: 0\n",
      "Standard Deviation: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Plot data distribution\n",
    "dlt.utils.plot_distribution_data(Y=data.train_labels,\n",
    "                                 dataset_name='y_train',\n",
    "                                 classes=data.classes,\n",
    "                                 fname='dist_train.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='dist_train.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、各ラベルに対して同じだけのデータ（厳密に言えば正解ラベル）が存在していることがわかります。実際、データセットを自分で作るときにこのようにしないと、最終的な結果にラベルのバイアスがかかってしまいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・・・このままでは各データを扱いにくいので、RGBの値(白なら255、黒なら0)を利用して0から1までの間に**正規化**することにします。この操作により`X_train, X_test`の形状は変わりません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### one-hotベクトルについて\n",
    "\n",
    "ところで、分類問題では最後のActivationにsoftmaxを使います。というのは、出力値のリストの添字を見ることで、そのデータがどのカテゴリーに分類されうるか(その割合or確率)がわかるからです。たとえば適当な5つのカテゴリー問題を考えたとして\n",
    "\n",
    "```\n",
    "[0.03, 0.05, 0.85, 0.03, 0.04]\n",
    "```\n",
    "なるリストが得られたとすると、`np.argmax([0.03, 0.05, 0.85, 0.03, 0.04])=[2]`であるから結果として「カテゴリー`2`の確率が0.85x100=85%程度であるらしい」ことがわかります。\n",
    "\n",
    "このように添字を一番大きいところを取ればそのカテゴリーがわかります。逆に言えば、正解ラベルだけ任意の数字を振ってほかと区別できるようにしておけば、そのラベルを見つければ良いのでコンピュータもそれがカテゴリーであることがわかりますよね。と考えると、正解ラベルに1, それ以外に0を振っておけば、1がある場所その添字を見れば正解かどうか分かることになります。これをone-hotベクトルと言います。具体例で考えてみます。\n",
    "\n",
    "上で表したリストを例に取って、正解ラベルを2であるとすると、このときのone-hotベクトルは\n",
    "```\n",
    "[0, 0, 1, 0, 0]\n",
    "```\n",
    "となります。`1`のある場所`2`がその画像の属するカテゴリーであることを示しています。今回は10種類のカテゴリーの分類を扱うので、10個の要素を持つone-hotベクトルに分類しておきます。(いくつの分類がほしいかはここで決める。)\n",
    "\n",
    "`one_hot`ベクトルへの変換メソッドは`dlt`内にも用意されています。これはNumpyでも実装できるんですね。ただまぁKerasでも`to_categorical`が用意されているので、こちらを使うことにします。ちなみにKerasの`to_categorical`はNumpyオブジェクトとして出力されるので、同じものです。\n",
    "\n",
    "まず読み込んだ生のデータを見てみますと、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " [4]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [7]\n",
      " [8]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "print(data.train_labels[:10]) #はじめの10個の画像ラベル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "となっています。一枚目の画像`6`については先程見たとおり`frog`でしたね。それではこれをone-hotベクトルに直すとはどういうことかを考えてみます。ラベルが`6`というのは、「10個の要素を持つタプル（カテゴリーが10個あるので)のうち、添字6番目の要素が`1`, それ以外が`0`」ということを意味しています。つまり、`[6]`というのは\n",
    "\n",
    "```\n",
    "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "```\n",
    "\n",
    "というone-hotベクトルに対応します。このようにすれば、テスト用のデータをネットワークに通して予測させた後(`predict`)の結果は、その確率が一番大きいところ、すなわち`1`の場所がどこにあるかを調べれば良いことになりますね。このようにone-hotベクトルになおしておけば、softmaxからも解釈が可能になりカテゴリーの概念がよりコンピュータに理解させやすくなります。\n",
    "\n",
    "※出力値も入力値に対応させて`Y_train`, `Y_test`と置くことにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ということで、訓練用`Y_train`とテスト用`Y_test`の出力値をone-hotベクトルに直しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "Y_train = to_categorical(data.train_labels, 10)\n",
    "Y_test = to_categorical(data.test_labels, 10)\n",
    "\n",
    "print(type(Y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "つまり用途としては\n",
    "\n",
    "- `X_train`, `Y_train` => 学習させて、ウエイトの値を最適化法で求めて学習させる。\n",
    "\n",
    "- `X_test`, `Y_test` => 学習させたデータを評価\n",
    "\n",
    "となっています。結果として何を計算させたのかについては後で述べます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layerの構造\n",
    "\n",
    "ここではネットワークを構築します。Tensorflowを触れた方はご存知かと思いますが、[ネットワークの構築と学習の際のデータのインプットは別個のものとして扱われます](https://qiita.com/hiroyuki827/items/509c2ac7735c8c16ad45)。Kerasも同様で、まずグラフを構築し、その後学習させる際にデータを流すという手順を踏みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、出力層(output layer)のニューロンは10個ですね。これは10個に分類したいことに対応しています。この数は特別な値なので、別に宣言しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回は基本的な構造しか扱わないので、[`Sequential`モデル](https://keras.io/ja/models/sequential/)を扱います。インスタンス`model`を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これ以降はこのインスタンス`model`にlayerを追加していけばいいので、以下のように設定します。このネットワークの設定の仕方は、全く任意です。[Kerasのサンプルコード](https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py)を見ると、少し違ったコードが書かれています。実際、サンプルコードのほうを実行すると、より高い精度が得られます。今回は説明の都合上、低い精度を与えるニューラルネットワーク構造を考えてみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layer 1\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# CNN layer 2\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# output\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後の`Activation`は必ず`softmax`です。一応参考書籍ではsoftmax層はいらないとか書いてあるのですが、まぁわかりやすさを考慮して付け加えておくことにします。(3行程度変わらない!)\n",
    "\n",
    "以上のように構成したCNNは以下の構造をとっています。`Flatten()`によって`output shape`が１次元になっていることがわかりますね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 15, 15, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 学習させる\n",
    "次に、上のように構築したニューラルネットワークに対して具体的にデータを入れて学習させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "どのように学習させるかという情報は、[`compile`メソッド](https://keras.io/ja/models/sequential/#compile)で指定できます。`model`インスタンスに指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "loss='categorical_crossentropy', # 損失関数の設定\n",
    "optimizer=Adam(lr=0.001), # 最適化法の指定\n",
    "metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずネットワークの最適化法を決めます。分類問題では`categorical_crossentropy`（交差エントロピー）を使います（上記書籍参照）。最適化法は`Adam`を使うことにしましょう。学習率は`lr=0.001`としておきます。（これはHyperparameterなので、結果を見ながら自分で決める必要があります。)\n",
    "\n",
    "最適化法については: [Optimizer : 深層学習における勾配法について](http://qiita.com/tokkuman/items/1944c00415d129ca0ee9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "いよいよ学習させることにします。学習は`fit`メソッドで実行します。入力値と最終的な値の学習データを引数に入れます。バッチ数は大きい方が精度が高くなるのですが、メモリーが少ないとリソースエラーになってしまうので、程々の値としてここでは`128`にしておきます。(GPU上の計算でResource Errorになった場合は、この値が大きいことが原因であることが多いです。）エポック数も`40`です。バッチ数が128, エポック数40ということで、1エポックあたり全インプットデータを128ずつ取り出して計算を繰り返します。全データを取り終えたら1エポックの終了です。\n",
    "\n",
    "また`validation_split`というのは、学習データの何パーセントを最終的なパラメータの設定に使うかを指定するものです。このデータにより、過学習しているかどうかを知ることができます。この設定のほかに個別に`validation_data`を設定すればより高精度の結果を得ることができるようですが、このチュートリアルではそれを目的としていないので、とりあえず10%を使うということにしておきます。(`validation_data`を使うときは`X_train`を適当に(`X_valid`, `Y_valid`などと分割する必要があります。)　\n",
    "\n",
    "混乱を防ぐためにまとめておくと、\n",
    "\n",
    "```\n",
    "- X_train, Y_train => 学習データ. ウエイトとバイアスが決まったら学習終了\n",
    "- X_test, Y_test => テストデータ. 学習後に分類できてるか確かめる。\n",
    "- X_valid, Y_valid (もし`validation_data`使うなら) => 過学習のチェック. 適宜ハイパーパラメータを調整する。\n",
    "```\n",
    "となっています。以下で見るように、過学習をチェックするためには`val_loss`もしくは`val_acc`を追いかける必要があります。これらは上のいずれかを有効にすることで可能になります。\n",
    "\n",
    "なお、メモリーの容量の関係から、`X_train`のデータをまるまる用いて計算させることはできません。その中のいくつかを`batch_size`で取り出して、計算させます。すべてのデータを一通り計算させたとき、それを1 epochと数えます。たとえば`batch_size=32`とすると、1 epoch内で`50000 (images) / 32 (images) = 1562`回だけ計算させることになります。1562回終わったら、1 epochです。\n",
    "\n",
    "更に人間が決めるパラメータとして、何epoch学習させるかというものがあります。ここでは適当に`epochs=40`と設定しておきます。この回数は多すぎても時間がかかりすぎるだけですし（過学習が起きます）、少なすぎては学習が足りなくなってしまいます。さらにモデルによっても異なるので、どうすればいいかは少しづつ変えてみるしか無いのですが、Kerasでは`callbacks`の`EarlyStopping`が使えます。`EarlyStopping`の使い方は練習問題に回すとして、余力があれば色々変えて学習がどう進んでいくか見てみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の情報を[`fit`メソッド](https://keras.io/ja/models/sequential/#fit)に入れます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/40\n",
      "45000/45000 [==============================] - 227s 5ms/step - loss: 1.6825 - acc: 0.3799 - val_loss: 1.3348 - val_acc: 0.5204\n",
      "Epoch 2/40\n",
      "45000/45000 [==============================] - 213s 5ms/step - loss: 1.2799 - acc: 0.5411 - val_loss: 1.0953 - val_acc: 0.6106\n",
      "Epoch 3/40\n",
      "45000/45000 [==============================] - 212s 5ms/step - loss: 1.0972 - acc: 0.6122 - val_loss: 0.9410 - val_acc: 0.6662\n",
      "Epoch 4/40\n",
      "45000/45000 [==============================] - 247s 5ms/step - loss: 0.9805 - acc: 0.6526 - val_loss: 0.8497 - val_acc: 0.7110\n",
      "Epoch 5/40\n",
      "45000/45000 [==============================] - 223s 5ms/step - loss: 0.8965 - acc: 0.6846 - val_loss: 0.8329 - val_acc: 0.7160\n",
      "Epoch 6/40\n",
      "45000/45000 [==============================] - 208s 5ms/step - loss: 0.8231 - acc: 0.7086 - val_loss: 0.7515 - val_acc: 0.7372\n",
      "Epoch 7/40\n",
      "45000/45000 [==============================] - 223s 5ms/step - loss: 0.7768 - acc: 0.7261 - val_loss: 0.6848 - val_acc: 0.7666\n",
      "Epoch 8/40\n",
      "45000/45000 [==============================] - 217s 5ms/step - loss: 0.7264 - acc: 0.7422 - val_loss: 0.6726 - val_acc: 0.7650\n",
      "Epoch 9/40\n",
      "45000/45000 [==============================] - 224s 5ms/step - loss: 0.6867 - acc: 0.7579 - val_loss: 0.6413 - val_acc: 0.7824\n",
      "Epoch 10/40\n",
      "45000/45000 [==============================] - 211s 5ms/step - loss: 0.6569 - acc: 0.7681 - val_loss: 0.6400 - val_acc: 0.7844\n",
      "Epoch 11/40\n",
      "45000/45000 [==============================] - 206s 5ms/step - loss: 0.6316 - acc: 0.7780 - val_loss: 0.6219 - val_acc: 0.7884\n",
      "Epoch 12/40\n",
      "45000/45000 [==============================] - 200s 4ms/step - loss: 0.6005 - acc: 0.7885 - val_loss: 0.6431 - val_acc: 0.7838\n",
      "Epoch 13/40\n",
      "45000/45000 [==============================] - 220s 5ms/step - loss: 0.5777 - acc: 0.7945 - val_loss: 0.6010 - val_acc: 0.8002\n",
      "Epoch 14/40\n",
      "45000/45000 [==============================] - 241s 5ms/step - loss: 0.5554 - acc: 0.8038 - val_loss: 0.6028 - val_acc: 0.7986\n",
      "Epoch 15/40\n",
      "45000/45000 [==============================] - 216s 5ms/step - loss: 0.5292 - acc: 0.8124 - val_loss: 0.5978 - val_acc: 0.8012\n",
      "Epoch 16/40\n",
      "45000/45000 [==============================] - 223s 5ms/step - loss: 0.5136 - acc: 0.8165 - val_loss: 0.6232 - val_acc: 0.7968\n",
      "Epoch 17/40\n",
      "45000/45000 [==============================] - 208s 5ms/step - loss: 0.5032 - acc: 0.8206 - val_loss: 0.5959 - val_acc: 0.7988\n",
      "Epoch 18/40\n",
      "45000/45000 [==============================] - 211s 5ms/step - loss: 0.4798 - acc: 0.8282 - val_loss: 0.6210 - val_acc: 0.7938\n",
      "Epoch 19/40\n",
      "45000/45000 [==============================] - 209s 5ms/step - loss: 0.4716 - acc: 0.8328 - val_loss: 0.5747 - val_acc: 0.8032\n",
      "Epoch 20/40\n",
      "45000/45000 [==============================] - 216s 5ms/step - loss: 0.4512 - acc: 0.8389 - val_loss: 0.5871 - val_acc: 0.8040\n",
      "Epoch 21/40\n",
      "45000/45000 [==============================] - 204s 5ms/step - loss: 0.4312 - acc: 0.8476 - val_loss: 0.6208 - val_acc: 0.7978\n",
      "Epoch 22/40\n",
      "45000/45000 [==============================] - 192s 4ms/step - loss: 0.4262 - acc: 0.8488 - val_loss: 0.5934 - val_acc: 0.8056\n",
      "Epoch 23/40\n",
      "45000/45000 [==============================] - 189s 4ms/step - loss: 0.4179 - acc: 0.8516 - val_loss: 0.5731 - val_acc: 0.8132\n",
      "Epoch 24/40\n",
      "45000/45000 [==============================] - 190s 4ms/step - loss: 0.4111 - acc: 0.8530 - val_loss: 0.5838 - val_acc: 0.8094\n",
      "Epoch 25/40\n",
      "45000/45000 [==============================] - 189s 4ms/step - loss: 0.3981 - acc: 0.8588 - val_loss: 0.6056 - val_acc: 0.8040\n",
      "Epoch 26/40\n",
      "45000/45000 [==============================] - 189s 4ms/step - loss: 0.3884 - acc: 0.8624 - val_loss: 0.6217 - val_acc: 0.8012\n",
      "Epoch 27/40\n",
      "45000/45000 [==============================] - 219s 5ms/step - loss: 0.3771 - acc: 0.8653 - val_loss: 0.5868 - val_acc: 0.8106\n",
      "Epoch 28/40\n",
      "45000/45000 [==============================] - 260s 6ms/step - loss: 0.3726 - acc: 0.8675 - val_loss: 0.6102 - val_acc: 0.8056\n",
      "Epoch 29/40\n",
      "45000/45000 [==============================] - 260s 6ms/step - loss: 0.3615 - acc: 0.8710 - val_loss: 0.6162 - val_acc: 0.8062\n",
      "Epoch 30/40\n",
      "45000/45000 [==============================] - 260s 6ms/step - loss: 0.3525 - acc: 0.8736 - val_loss: 0.6009 - val_acc: 0.8160\n",
      "Epoch 31/40\n",
      "45000/45000 [==============================] - 261s 6ms/step - loss: 0.3488 - acc: 0.8747 - val_loss: 0.5900 - val_acc: 0.8152\n",
      "Epoch 32/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3443 - acc: 0.8768 - val_loss: 0.6067 - val_acc: 0.8108\n",
      "Epoch 33/40\n",
      "45000/45000 [==============================] - 259s 6ms/step - loss: 0.3437 - acc: 0.8788 - val_loss: 0.6084 - val_acc: 0.8096\n",
      "Epoch 34/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3292 - acc: 0.8834 - val_loss: 0.6204 - val_acc: 0.8072\n",
      "Epoch 35/40\n",
      "45000/45000 [==============================] - 257s 6ms/step - loss: 0.3266 - acc: 0.8843 - val_loss: 0.5963 - val_acc: 0.8130\n",
      "Epoch 36/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3272 - acc: 0.8837 - val_loss: 0.6000 - val_acc: 0.8158\n",
      "Epoch 37/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3197 - acc: 0.8853 - val_loss: 0.6156 - val_acc: 0.8152\n",
      "Epoch 38/40\n",
      "45000/45000 [==============================] - 259s 6ms/step - loss: 0.3132 - acc: 0.8879 - val_loss: 0.5987 - val_acc: 0.8128\n",
      "Epoch 39/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3098 - acc: 0.8884 - val_loss: 0.6110 - val_acc: 0.8136\n",
      "Epoch 40/40\n",
      "45000/45000 [==============================] - 258s 6ms/step - loss: 0.3105 - acc: 0.8894 - val_loss: 0.6090 - val_acc: 0.8114\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(X_train, Y_train,\n",
    "              batch_size=128,\n",
    "              epochs=40,\n",
    "              verbose=1,\n",
    "              validation_split=0.1 # 今回は訓練データセットの10%をvalidationデータセットとして使う\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最終的に学習させた結果として、およそ81.14%の精度で分類できることがわかりました。え？88.94%じゃないかって？その疑問はあとに取っておいてください。\n",
    "\n",
    "<u>注意</u> val_accについては、訓練用のデータを用いて精度を見ているだけなので、テストデータ(そのモデルが実用可能かどうかを用いて評価したもの(model.evaluate)とは意味合いが異なります。詳しくは後述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、学習されたモデルがどの程度の精度を持ってテストデータを判定できるか(分類できるか)を評価することにします。これは[`evaluate`メソッド](https://keras.io/ja/models/sequential/#compile)で実現できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6653376616477966\n",
      "Test accuracy: 0.7973\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test,\n",
    "                    verbose=0\n",
    "                    )\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このように、学習させた結果を用いてテストデータを分類させると、75.35%の精度で正しく分類されることがわかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なお、この結果を後から使いたいときは、以下のようにモデルを保存することができます。モデルの再利用については、付録Aを参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'results'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "model.save(os.path.join(folder, 'my_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 出力結果を見る\n",
    "\n",
    "ここでは以上の結果をグラフを用いて見ていくことにします。dltパッケージがようやく本領発揮します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 分類チェック"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dltでは訓練されたデータと、その予測値を各画像ごとに表示するコードが書かれています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習させたモデルから分類ラベルを取り出すには、[`predict`メソッド](https://keras.io/ja/models/sequential/#predict)が使えます。引数にはテストデータを指定します。さらにここでは最終的な値ではなく、`softmax`の出力するリストの最大値のラベルがほしいので、`np.argmax`でその添字を取り出します。各ラベルごとの精度を`preds`, 予測されるであろうラベルのarrayを`cls`とします。\n",
    "\n",
    "**コメント** 私自身`predict`が実際に何をやっているのかわからなかったので、ここで詳しくまとめておきます。\n",
    "最初に、`X_train, Y_train`を用いてネットワークを学習させました。この学習で80%の精度を得たわけですが、結果として得られたものは各層の重み、バイアスです。これが入力データ`X_train, Y_train`にあうように、そして**汎化性**(あとで述べます)を持つように決まったわけです。これが学習で得たものの実態です。次に、学習で得たネットワーク全体の構造を（文字通り）通して得られた画像(とか実体としてのデータ)がどう得られるかを見たい。(⇐今ここ) それをするのが`model.predict(X_test)`なんですね。`model`は学習されたネットワークを指定し、引数の`X_test`はテストデータをネットワークに通すために指定しているわけです。それで得られたものは`Yp`です。したがって、`Yp`は`Y_train`や`Y_test`と同じデータ構造(タプルとして)を持っていることになります。\n",
    "\n",
    "まとめると、以下で比べるのは「学習されたネットワークを通した入力値(テストデータ)」と「実際のテストデータ」の違いです。もっというと、10個の要素を持つリストの中で、一番大きい確率を持つ要素の添字が一緒かどうかを見ています。(このあたりの内容は可視化しなくてもわかります: [自前の保存したモデルを用いて画像を分類してみる。](http://qiita.com/hiroyuki827/items/e6914b9b85f7bfe55078))\n",
    "\n",
    "ともかく以下のように実装すれば"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities for the test set\n",
    "preds = model.predict(X_test)\n",
    "cls = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**: ここでは新しいバージョンのKerasに実装された新しいメソッド`predict_classes`を用いることにします。ただしやっていることは`np.argmax(preds,axis=1)`と全く同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから以下のデータが取り出せます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some test images along with the prediction\n",
    "for i in range(10):\n",
    "    dlt.utils.plot_prediction(\n",
    "        preds[i],\n",
    "        data.test_images[i],\n",
    "        data.test_labels[i],\n",
    "        data.classes,\n",
    "        fname=os.path.join(folder, 'test-%i.png' % i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='results/test-0.png'/>\n",
    "<img src='results/test-1.png'/>\n",
    "<img src='results/test-2.png'/>\n",
    "<img src='results/test-3.png'/>\n",
    "<img src='results/test-4.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "オレンジ色のついたラベルは、写真の正しいカテゴリーを示しており、バーの値は学習によるコンピュータの分類の精度を表しています。(どのくらいの確率で、その写真がそのカテゴリーに分類されるか。) テスト用の画像に対して、どの程度予測できているかがわかりますね。上２つの画像の分類は正しいですが、蛙(frog)に関しては分類が間違っていますね。\n",
    "\n",
    "たとえば一つ目の画像ですが、正しいラベルは猫(cat)です。しかしながらコンピュータは数%の確率で犬(dog)だと考えています。まぁ90%以上の精度で猫だと言っているので間違いではありません。これは正しい分類!(人間でも間違えることありますよね?)\n",
    "\n",
    "これとは対照的なのが二つ目の画像で、90%の確率で船(ship)だとあてていますが、10%程度でautomobileと分類してしまっています。。。\n",
    "\n",
    "こんな感じで、テスト用の画像に対して、どの程度予測できているかがわかりますね。\n",
    "\n",
    "**これが10個続きます**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrixのほうは以下のようになっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_confusion_matrix(data.test_labels, cls, data.classes,\n",
    "                                  title='confusion matrix',\n",
    "                                  fname=os.path.join(folder, 'confusion_matrix.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='results/confusion_matrix.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "と出力されます。これにより、学習によってどのカテゴリーに、より精度が高く(もしくは低く)分類されたかを知ることができます。上の結果をみると、犬と猫が精度が低いようです。。。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 損失関数, 精度, 過学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に学習結果の確率と損失関数を見ます。以下のコードは実行する際は、いずれかをコメントアウトしてくださいね。\n",
    "\n",
    "よく精度~%が得られたという意味で、精度が非常に重要視されるように見受けられるのですが、個人的にはloss functionの動きのほうがよっぽど大事だと思います。損失関数を見ずに確率だけを見ていた場合、その学習結果は**過学習**に陥っている可能性があるからです。どんな場合でも、損失関数の動きを見るようにしてください。\n",
    "\n",
    "以下のコードでは、`fit.history['loss']`と`fit.history['val_loss']`をグラフにしています。`val_loss`, `val_acc`は`validation_split`を入れると自動的に生成されるラベルで、学習のインスタンス`fit`を用いて`fit.history[]`で呼び出せます。それぞれの出力結果は以下のようになっています。(精度についても`fit.history['acc']`と`fit.history['val_acc']`でできます。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlt.utils.plot_loss_and_accuracy(fit,  #model.fitのインスタンス\n",
    "                                   fname=os.path.join(folder, 'cifar10-tutorial.png') #保存するファイル名とパス\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='results/cifar10-tutorial.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Loss\n",
    "\n",
    "まず損失関数の方を見ていきたいのですが、途中から`val_loss`が増加に転じていますね。これが過学習の兆候です！これ以上学習させてはいけません。\n",
    "過学習は訓練用のデータに以上にフィットしていしまい、汎用化できなくなる現象です。\n",
    "\n",
    "<img src='images/overfitting.png'/>\n",
    "\n",
    "`X_train`のうち90%のデータを訓練させた結果、訓練用のデータの10%を使っても、76%の精度にとどまってしまうことを示しています。というわけで実際の精度は90%ではなく、76%(もしくはテストデータで測った72%)ということになります。過学習を止めるには、層を厚くするとか、Dropoutを入れるとか、いろいろな方法があります。ここではそんなに高い精度は求めていないので、議論しません。\n",
    "\n",
    "Kerasでは`Early stopping`というメソッドが使えるので、これを使えば`val_loss`が増加に転じるところ、つまり最小値で学習を止めることができます。今回はデータを見ればそんなに学習させなくてもどこで止めるべきかわかるので使いませんでした。\n",
    "\n",
    "損失関数を見れば過学習がわかると言いましたが、もちろん精度を見てもわかります。\n",
    "\n",
    "#### Model Accuracy\n",
    "右側のグラフを御覧ください。`val_acc`が途中から上昇を止めていますね。これも過学習が起きている証拠です。また`Early stopping`を用いる場合は、最大値になるところで止めれば良いことがわかりますね。\n",
    "\n",
    "*精度だけ見ると早とちりしそうだけど、損失関数もみてやればじっくり見れる気がする...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*参考*: このようなグラフからの判断は一概に方法があるわけではないので、非常にどこがおかしいのかを知るのは難しいのですが、個人的に以下のような記事を書いてみました: [ハイパーパラメータをいじってグラフを見てみる](http://qiita.com/hiroyuki827/items/213146d551a6e2227810)　"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 全体のコード"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで議論してきた全体のコードは以下のようになっています。(講義用からの転載なのでコメントは英語)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import dlt\n",
    "import os\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import rmsprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Load and preprocess data\n",
    "# ---------------------------------------------------------\n",
    "data = dlt.cifar.load_cifar10()\n",
    "\n",
    "# plot some example images\n",
    "dlt.utils.plot_examples(data, fname='examples.png')\n",
    "print(data.train_images.shape)\n",
    "print(data.train_labels.shape)\n",
    "print(data.test_images.shape)\n",
    "print(data.test_labels.shape)\n",
    "\n",
    "# preprocess the data in a suitable way\n",
    "# reshape the image matrices to vectors\n",
    "#RGB 255 = white, 0 = black\n",
    "X_train = data.train_images.reshape([-1, 32, 32, 3])\n",
    "X_test = data.test_images.reshape([-1, 32, 32, 3])\n",
    "print('%i training samples' % X_train.shape[0])\n",
    "print('%i test samples' % X_test.shape[0])\n",
    "print(X_train.shape)\n",
    "\n",
    "# convert integer RGB values (0-255) to float values (0-1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "# convert class labels to one-hot encodings\n",
    "Y_train = to_categorical(data.train_labels, 10)\n",
    "Y_test = to_categorical(data.test_labels, 10)\n",
    "\n",
    "# Plot data distribution\n",
    "dlt.utils.plot_distribution_data(Y=data.train_labels,\n",
    "                                 dataset_name='y_train',\n",
    "                                 classes=data.classes,\n",
    "                                 fname='dist_train.png')\n",
    "                                 \n",
    "# ----------------------------------------------------------\n",
    "# Model and training\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "# model\n",
    "model = Sequential()\n",
    "\n",
    "# CNN layer 1\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# CNN layer 2\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# output\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(\n",
    "loss='categorical_crossentropy',\n",
    "optimizer=rmsprop(lr=0.0001, decay=1e-6),\n",
    "metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(X_train, Y_train,\n",
    "              batch_size=32,\n",
    "              epochs=50, #shouldn't be raised to 100, because the overfitting occurs.\n",
    "              verbose=2,\n",
    "              validation_split=0.1\n",
    "                )\n",
    "\n",
    "score = model.evaluate(X_test, Y_test,\n",
    "                    verbose=0\n",
    "                    )\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# ----------------------------------------------\n",
    "# Some plots\n",
    "# ----------------------------------------------\n",
    "\n",
    "# make output directory\n",
    "folder = 'results'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "model.save(os.path.join(folder, 'my_model.h5'))\n",
    "\n",
    "# predicted probabilities for the test set\n",
    "preds = model.predict(X_test)\n",
    "cls = model.predict_classes(X_test)\n",
    "                                  \n",
    "# plot some test images along with the prediction\n",
    "for i in range(10):\n",
    "    dlt.utils.plot_prediction(\n",
    "        preds[i],\n",
    "        data.test_images[i],\n",
    "        data.test_labels[i],\n",
    "        data.classes,\n",
    "        fname=os.path.join(folder, 'test-%i.png' % i))\n",
    "\n",
    "# plot the confusion matrix\n",
    "dlt.utils.plot_confusion_matrix(data.test_labels, cls, data.classes,\n",
    "                                  title='confusion matrix',\n",
    "                                  fname=os.path.join(folder, 'confusion matrix.png'))\n",
    "\n",
    "# plot the loss and accuracy graph\n",
    "dlt.utils.plot_loss_and_accuracy(fit,  #model.fitのインスタンス\n",
    "                                   fname=os.path.join(folder, 'cifar10-tutorial.png') #保存するファイル名とパス\n",
    "                                  )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 宿題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまですべて完成したコードを示してきました。なので以下の問題は意味が無いのかもしれませんが、一通りやってみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- どのようにすれば精度が高いCNNが得られるでしょうか？90%以上の精度(もちろん過学習なし)の実装をしてください。その場合、`Earlystopping`を用いて過学習が起きていないことをグラフとともに示し、議論してください。必要ならばその他のCallbackを使いなさい。(4 points)\n",
    "\n",
    "- confusion matrixを出力し、それからわかることを議論してください。(3 points)\n",
    "\n",
    "- 予測されたカテゴリーに対して、間違って分類された画像はいくつあるでしょうか？それらに共通してわかることはなにでしょうか？なぜそのような(分類ミス）が起こったのでしょうか？ (2 points)\n",
    "\n",
    "- [Bonus] 付録A.3 を参考にして、カテゴリー外の画像ならどうなるか議論しなさい。また、ImageNetを用いた画像を判定するコードを公式ドキュメントを参考にして実装し、その画像が正しく分類されることを確認しなさい。 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 終わりに"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "少々中途半端なチュートリアルになってしまいましたが、CNNやKerasの魅力をわかっていただけるとこれ以上の喜びはありません。もっともっとKerasユーザーが増え、Kerasの発展に貢献していくことを願います。\n",
    "\n",
    "なお、実際は１からネットワークを構築して学習せずとも、すでに学習させたモデルのネットワークを用いて学習させれば、より少ないリソースで高い精度の結果を得ることができます。これを**転移学習**と言います。またの機会にこの学習について触れることにします。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "390px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": "3",
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
